{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders are variables that we feed Tensorflow each time we want to run a computation\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are different from placeholders since they live between computations, \n",
    "# and these computations can change the variables. Weights and biases are typically\n",
    "# set as variables\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Variables need to be initialized within a session before they can be used (not sure why)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward propagation is easy..\n",
    "y = tf.matmul(x,W) + b\n",
    "\n",
    "# Defining the loss function\n",
    "\n",
    "# tf.nn.softmax_cross_entropy_with_logits applies the softmax on the model's unnormalized model prediction \n",
    "# (each of the 10 possible results) and sums over all classes (digits 0-10)\n",
    "# tf.reduce_mean takes the average over these sums. (Is this summing over an epoch? Makes sense to take \n",
    "# the average of that..)\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# Tensorflow uses automatic differentiation (still unsure what this is exactly.) \n",
    "# to find the gradients of the loss with respect to each of the variables\n",
    "\n",
    "# Going to use steepest gradient descent to train this one. Tensorflow apparently has a variety of built-in\n",
    "# optimization algorithms. (may want to check this out)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Tutorial says that the above line adds a few new operations to the computation graph (there is a way to view\n",
    "# this if I've understood correctly using TensorBoard) that include computing gradients, computing\n",
    "# the parameter updates, and applying the updates to the parameters.\n",
    "\n",
    "# train_step will apply the gradient descent updates to the parameters. Training is accomplished by repeatedly\n",
    "# running train_step.\n",
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9175\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# y is the prediction, y_ is the true value\n",
    "# tf.argmax finds the index of the highest value of the tensor along the specified axis\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_,1))\n",
    "\n",
    "# correct_prediction is a boolean array. The code below recasts \"True\" to 1 and \"False\" to 0.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# prints the percent accuracy of the trained network.\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The neuron type used for the convolutional layers is different from the sigmoid one. See ReLU (rectifier neurons)\n",
    "# The code below initializes the weights and biases for the convolutional part of the network\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution and pooling (downsampling the output image by applying some operation on a slice (from what I\n",
    "# understand, a slice is the output image of a filter) e.g. taking the maximum of 2x2 squares) \n",
    "# they use the term \"Stride size\" (how much the filter\n",
    "# moves between calculations of a pixel in the output) and \"padding\" (applying filters shrinks the size of the image\n",
    "# and padding keeps the image a little larger by adding zeros. In the case below, I think it's keeping the 28x28\n",
    "# size between convolutions).\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First convolutional layer\n",
    "\n",
    "# The first layer will compute 32 features for a 5x5 patch.\n",
    "# The arguments here are the patch size: 5,5; the number of input channels (color?): 1; \n",
    "# and the number of output channels: 32\n",
    "\n",
    "# (I'm not so sure about the rationale here. Why 32 features from 25 pixels?)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# the final dimension here is the # of color channels..\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second convolutional layer\n",
    "\n",
    "# This layer will have 64 features for each 5x5 patch..\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Densely connected layer\n",
    "\n",
    "# The tutorial notes that at this point the image has been reduced to 7x7. The next layer will be a fully connected\n",
    "# layer with 1024 neurons.\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "\n",
    "# to reduce overfitting. Tensorflow automates the process of scaling the neuron activation and masking them. Not\n",
    "# even sure how dropout (computes a probability that a neuron's output is kept) helps.\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Readout layer\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, training accuracy 0.1\n",
      "step: 100, training accuracy 0.8\n",
      "step: 200, training accuracy 0.88\n",
      "step: 300, training accuracy 0.88\n",
      "step: 400, training accuracy 0.86\n",
      "step: 500, training accuracy 0.94\n",
      "step: 600, training accuracy 1\n",
      "step: 700, training accuracy 0.96\n",
      "step: 800, training accuracy 0.92\n",
      "step: 900, training accuracy 0.98\n",
      "step: 1000, training accuracy 1\n",
      "step: 1100, training accuracy 0.9\n",
      "step: 1200, training accuracy 1\n",
      "step: 1300, training accuracy 1\n",
      "step: 1400, training accuracy 0.92\n",
      "step: 1500, training accuracy 0.98\n",
      "step: 1600, training accuracy 0.96\n",
      "step: 1700, training accuracy 0.96\n",
      "step: 1800, training accuracy 1\n",
      "step: 1900, training accuracy 1\n",
      "test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate\n",
    "\n",
    "# Things that are different from the previous part:\n",
    "# -Using a different optimizer here called ADAM (tutorial says it's more sophisticated)\n",
    "# -have an additional parameter, keep_prob, to control the dropout rate\n",
    "# -Add logging to every 100th iteration\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print('step: %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    print('test accuracy: %g' % accuracy.eval(feed_dict={\n",
    "        x: batch[0], y_: batch[1], keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
